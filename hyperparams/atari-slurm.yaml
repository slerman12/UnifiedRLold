defaults:
  - _self_
  #  - task@_global_: dmc/quadruped_walk
  #  - task@_global_: brax/brax_halfcheetah-v0
  - task@_global_: atari/breakout
  - override hydra/launcher: submitit_slurm


# environments/domain
envs: atari
# task settings
frame_stack: 4
action_repeat: 4
## see section 4.1 in https://arxiv.org/pdf/1812.06110.pdf
#terminal_on_life_loss: true  # true by default
discount: 0.99
# train settings
num_train_frames:  1000001
num_seed_frames: 1600  # should be >= replay_buffer_num_workers * truncate_episode_len
#num_seed_frames: 4004  # should be >= replay_buffer_num_workers * truncate_episode_len + action_repeat ?
#num_seed_frames: 12000
#num_exploration_steps: 5000
max_episode_frames: 27000  # must be > update_every_steps, >= nstep - 1
truncate_episode_frames: 400
#truncate_episode_len: false
# eval
#eval_every_frames: 100000
#num_eval_episodes: 10  # would this take too long in atari?
eval_every_frames: 20000
num_eval_episodes: 10
# snapshot
save_snapshot: false
# replay buffer
replay_buffer_size: ${num_train_frames}
#store_every_frames: 1000  # should be below seed frames I think
#store_every_frames: false
#replay_buffer_num_workers: 2
replay_buffer_num_workers: 4
prioritized_replay: false
prioritized_replay_alpha: 0.6
nstep: 10
#batch_size: 256
batch_size: 32
# misc
seed: 1
#device: cpu
device: cuda
save_video: true
save_train_video: false
use_tb: true
# experiment
experiment: exp
# agent
lr: 1e-4
adam_eps: 0.00015
max_grad_norm: 10.0
feature_dim: 50

agent:
  _target_: agents.DRQLAgent
  obs_shape: ??? # to be specified later
  action_shape: ??? # to be specified later
  discrete: ??? # to be specified later
  device: ${device}
  lr: ${lr}
  adam_eps: ${adam_eps}
  max_grad_norm: ${max_grad_norm}
  critic_target_tau: 0.01
  min_eps: 0.1
  num_seed_frames: ${num_seed_frames}
  #  critic_target_update_frequency: 1
  #  critic_target_tau: 1.0
  intensity_scale: 0.05
  double_q: true
  dueling: true
  #  update_every_steps: 2
  use_tb: ${use_tb}
  #  num_expl_steps: 2000
  num_expl_steps: 5000
  #  num_expl_steps: 20000
  #  hidden_dim: 1024
  hidden_dim: 512
  feature_dim: ${feature_dim}
  prioritized_replay_beta0: 0.4
  prioritized_replay_beta_steps: ${num_train_frames}

hydra:
  run:
    dir: ./exp_local/${now:%Y.%m.%d}/${now:%H%M%S}_${hydra.job.override_dirname}
  sweep:
    dir: ./exp/${now:%Y.%m.%d}/${now:%H%M%S}_${agent._target_}_${experiment}
    subdir: ${hydra.job.num}
  launcher:
    timeout_min: 4300
    cpus_per_task: 4
#    gpus_per_node: 4
    tasks_per_node: 4
    mem_gb: 160
    nodes: 2
    partition: gpu
#    gres: gpu:4
    cpus_per_gpu: 16
#    gpus_per_task: 1
    constraint: K80
#    mem_per_gpu: null
#    mem_per_cpu: null
    submitit_folder: ./exp/${now:%Y.%m.%d}/${now:%H%M%S}_${agent._target_}_${experiment}/.slurm
